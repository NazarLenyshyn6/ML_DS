{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mini framework that implements auto differentiation of math expression defined with custom object Node.\\nAlso implements fully connected neural network model, optimizer, loss function.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Mini framework that implements auto differentiation of math expression defined with custom object Node.\n",
    "Also implements fully connected neural network model, optimizer, loss function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'tanh(((a * b) + c))' | Value: -0.9999999999999748 | Grad: 1.0\n",
      "Function: '((a * b) + c)' | Value: -16.0 | Grad: 5.040412531798211e-14\n",
      "Function: 'c' | Value: 5.0 | Grad: 5.040412531798211e-14\n",
      "Function: '(a * b)' | Value: -21.0 | Grad: 5.040412531798211e-14\n",
      "Function: 'b' | Value: -3.0 | Grad: 3.5282887722587475e-13\n",
      "Function: 'a' | Value: 7.0 | Grad: -1.5121237595394632e-13\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    \"\"\"Composed class for defining math expressions and automatic differentiation.\n",
    "\n",
    "    Args:\n",
    "        val (int | float): Value to store in Node object; will be used in mathematical operations.\n",
    "        label (str): String label to identify instance of Node.\n",
    "        grad (float): Gradient of current Node, default is 0.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        val: int | float, \n",
    "        label: str, \n",
    "        _components: tuple['Node'] = (),\n",
    "        ):\n",
    "        self.val = val\n",
    "        self.label = label\n",
    "        self.grad = 0\n",
    "        self._components = _components\n",
    "        self._backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Node(val={self.val})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Defines addition of two Nodes and sets up gradient computation.\n",
    "\n",
    "        Args:\n",
    "            other (Node or float or int): Node or numeric value to add.\n",
    "\n",
    "        Returns:\n",
    "            Node: Resultant Node with updated computation graph.\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Node) else Node(val=other, label=f'_{self.label}_')\n",
    "        new_node = Node(val=self.val + other.val, label=f'({self.label} + {other.label})', _components=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += new_node.grad\n",
    "            other.grad += new_node.grad\n",
    "        new_node._backward = _backward\n",
    "        return new_node\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Defines subtraction of two Nodes and sets up gradient computation.\n",
    "\n",
    "        Args:\n",
    "            other (Node or float or int): Node or numeric value to subtract.\n",
    "\n",
    "        Returns:\n",
    "            Node: Resultant Node with updated computation graph.\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Node) else Node(val=other, label=f'_{self.label}_')\n",
    "        new_node = Node(val=self.val - other.val, label=f'({self.label} - {other.label})', _components=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += new_node.grad\n",
    "            other.grad -= new_node.grad\n",
    "        new_node._backward = _backward\n",
    "        return new_node\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Defines multiplication of two Nodes and sets up gradient computation.\n",
    "\n",
    "        Args:\n",
    "            other (Node or float or int): Node or numeric value to multiply.\n",
    "\n",
    "        Returns:\n",
    "            Node: Resultant Node with updated computation graph.\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Node) else Node(val=other, label=f'_{self.label}_')\n",
    "        new_node = Node(val=self.val * other.val, label=f'({self.label} * {other.label})', _components=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += new_node.grad * other.val\n",
    "            other.grad += new_node.grad * self.val\n",
    "        new_node._backward = _backward\n",
    "        return new_node\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        \"\"\"Raises the Node to a given power and sets up gradient computation.\n",
    "\n",
    "        Args:\n",
    "            other (float): Power to which the node is raised.\n",
    "\n",
    "        Returns:\n",
    "            Node: Resultant Node with updated computation graph.\n",
    "        \"\"\"\n",
    "        new_node = Node(val=self.val ** other, label=f'({self.label}^{other})', _components=(self,))\n",
    "        def _backward():\n",
    "            self.grad += new_node.grad * (other * self.val ** (other - 1))\n",
    "        new_node._backward = _backward\n",
    "        return new_node\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other ** -1\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def _topological_sort(self):\n",
    "        \"\"\"Returns list of all Nodes forming the current Node in topological order.\"\"\"\n",
    "        topologically_sorted_nodes = []\n",
    "        traversed_nodes = set()\n",
    "        \n",
    "        def traverse(node):\n",
    "            \"\"\"Implements DFS for topological sorting.\"\"\"\n",
    "            if not node in traversed_nodes:\n",
    "                traversed_nodes.add(node)\n",
    "                for component in node._components:\n",
    "                    traverse(component)\n",
    "                topologically_sorted_nodes.append(node)\n",
    "        traverse(self)\n",
    "        return topologically_sorted_nodes\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        \"\"\"Applies sigmoid activation function to the current Node.\n",
    "\n",
    "        Returns:\n",
    "            Node: Node with sigmoid transformation applied.\n",
    "        \"\"\"\n",
    "        new_val = 1 / (1 + np.exp(-self.val))\n",
    "        new_node = Node(val=new_val, label=f'sigmoid({self.label})', _components=(self,))\n",
    "        def _backward():\n",
    "            self.grad += new_node.grad * (new_val * (1 - new_val))\n",
    "        new_node._backward = _backward\n",
    "        return new_node\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"Applies hyperbolic tangent activation function to the current Node.\n",
    "\n",
    "        Returns:\n",
    "            Node: Node with tanh transformation applied.\n",
    "        \"\"\"\n",
    "        new_val = (np.exp(self.val) - np.exp(-self.val)) / (np.exp(self.val) + np.exp(-self.val))\n",
    "        new_node = Node(val=new_val, label=f'tanh({self.label})', _components=(self, ))\n",
    "        def _backward():\n",
    "            self.grad += new_node.grad * (1 - new_val**2)\n",
    "        new_node._backward = _backward\n",
    "        return new_node\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Performs backpropagation to compute gradients.\"\"\"\n",
    "        self.grad = 1.0\n",
    "        topologicaly_sorted_nodes = self._topological_sort()\n",
    "        for node in topologicaly_sorted_nodes[::-1]:\n",
    "            node._backward()\n",
    "    \n",
    "    @property\n",
    "    def info(self):\n",
    "        \"\"\"Returns string with Node details.\"\"\"\n",
    "        return f\"Function: '{self.label}' | Value: {self.val} | Grad: {self.grad}\"\n",
    "    \n",
    "    @property\n",
    "    def computational_graph(self):\n",
    "        \"\"\"Prints the computational graph in reverse topological order.\"\"\"\n",
    "        topologicaly_sorted_nodes = self._topological_sort()\n",
    "        for node in topologicaly_sorted_nodes[::-1]:\n",
    "            print(node.info)\n",
    "    \n",
    "    \n",
    "# Example\n",
    "a = Node(val=7.0, label='a')\n",
    "b = Node(val=-3.0, label='b')\n",
    "c = Node(val=5.0, label='c')\n",
    "s = a*b + c\n",
    "o = s.tanh()\n",
    "o.backward()\n",
    "o.computational_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(val=0.7160178812275462)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Defines a single Neuron for a Neural Network.\"\"\"\n",
    "    \n",
    "    _activations = {\n",
    "        'sigmoid': lambda node: node.sigmoid(),\n",
    "        'tanh': lambda node: node.tanh()\n",
    "        }\n",
    "    \n",
    "    def __init__(self, n_inputs: int, activation: str = 'sigmoid'):\n",
    "        \"\"\"Initializes a Neuron.\n",
    "\n",
    "        Args:\n",
    "            n_inputs: Number of input connections.\n",
    "            activation: Activation function to use ('sigmoid' or 'tanh').\n",
    "        \"\"\"\n",
    "        self.w = [Node(random.uniform(-1, 1), label=f'neuron_w{i}') for i in range(n_inputs)]\n",
    "        self.b = Node(0, label='b')\n",
    "        self.activation = activation\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Computes output as activation(dot(w, x) + b).\n",
    "\n",
    "        Args:\n",
    "            x: List of input Node values.\n",
    "\n",
    "        Returns:\n",
    "            Activated output Node.\n",
    "        \"\"\"\n",
    "        activation = self._activations[self.activation]\n",
    "        return activation(sum([wi*xi for wi, xi in zip(self.w, x)]) + self.b)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Neuron(n_inputs={len(self.w)})'\n",
    "    \n",
    "    @classmethod\n",
    "    def _add_activation(cls, activation, activation_name):\n",
    "        \"\"\"Adds a new activation function to Neuron.\n",
    "\n",
    "        Args:\n",
    "            activation: Callable to use as activation function.\n",
    "            activation_name: Name to register the activation under.\n",
    "        \"\"\"\n",
    "        cls._activations[activation_name] = activation\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns a list of learnable parameters (weights and bias).\n",
    "\n",
    "        Returns:\n",
    "            List of Node instances (weights and bias).\n",
    "        \"\"\"\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class Layer:\n",
    "    \"\"\"Defines a Layer of a Neural Network.\n",
    "\n",
    "    Args:\n",
    "        n_inputs: Dimensionality of input.\n",
    "        n_outputs: Dimensionality of output.\n",
    "        activation: Activation function used in every node of Layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_inputs: int, \n",
    "        n_outputs: int,\n",
    "        activation: str = 'sigmoid'\n",
    "        ):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation = activation\n",
    "        self.neurons = [Neuron(n_inputs, activation) for _ in range(n_outputs)]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Layer(n_inputs={self.n_inputs}, n_outputs={self.n_outputs}, activation={self.activation})'\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Calculates the output of the Neurons in the Layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input values for the Layer.\n",
    "\n",
    "        Returns:\n",
    "            Output from the Layer; single value if only one neuron.\n",
    "        \"\"\"\n",
    "        out = [neuron(x) for neuron in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Returns a list of all learnable parameters in the Layer.\n",
    "\n",
    "        Returns:\n",
    "            List of Node instances (weights and biases).\n",
    "        \"\"\"\n",
    "        return [param for neuron in self.neurons for param in neuron.parameters()]\n",
    "    \n",
    "    \n",
    "class MLP:\n",
    "    \"\"\"Defines a fully connected Multi-Layer Perceptron.\n",
    "\n",
    "    Args:\n",
    "        n_inputs: Dimensionality of input.\n",
    "        hidden_layers: List of hidden layer sizes.\n",
    "        n_outputs: Dimensionality of output layer.\n",
    "        activations: Activation function(s) used in each layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, hidden_layers, n_outputs, activations = 'sigmoid'):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_outputs = n_outputs\n",
    "        total_dims = [self.n_inputs] + self.hidden_layers + [self.n_outputs]\n",
    "        self.activations = activations if not activations == 'sigmoid' else ['sigmoid' for _ in range(len(total_dims) - 1)]\n",
    "        self.layers = [\n",
    "            Layer(n_inputs=total_dims[i - 1], n_outputs=total_dims[i], activation=self.activations[i - 1]) \n",
    "            for i in range(1, len(total_dims))\n",
    "            ]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f'MLP(\\n'\n",
    "                f'    n_inputs={self.n_inputs},\\n'\n",
    "                f'    hidden_layers={self.hidden_layers},\\n'\n",
    "                f'    n_outputs={self.n_outputs},\\n'\n",
    "                f'    activations={self.activations}\\n'\n",
    "                f')')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input values.\n",
    "\n",
    "        Returns:\n",
    "            Output after forward pass.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Returns a list of all learnable parameters in the network.\n",
    "\n",
    "        Returns:\n",
    "            List of Node instances (weights and biases).\n",
    "        \"\"\"\n",
    "        return [param for layer in self.layers for param in layer.parameters()]\n",
    "\n",
    "# Example\n",
    "model = MLP(\n",
    "    n_inputs=2, \n",
    "    hidden_layers=[2], \n",
    "    n_outputs=1, \n",
    "    activations=['sigmoid', 'sigmoid']\n",
    "    )\n",
    "\n",
    "x = [1.0, 1.0]\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Optimizer(learning_rate=0.001)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Performs a gradient descent step on provided parameters.\n",
    "\n",
    "    Attributes:\n",
    "        parameters: List of parameters to update.\n",
    "        learning_rate: Step size for gradient descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, learning_rate = 0.001):\n",
    "        \"\"\"Initializes the Optimizer.\n",
    "\n",
    "        Args:\n",
    "            parameters: Callable that returns a list of parameters (e.g., model.parameters).\n",
    "            learning_rate: Learning rate for gradient update.\n",
    "        \"\"\"\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Optimizer(learning_rate={self.learning_rate})'\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Resets gradient of all parameters to zero.\"\"\"\n",
    "        for parameter in self.parameters(): parameter.grad = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"Performs one step of gradient descent update.\"\"\"\n",
    "        for parameter in self.parameters(): parameter.val -= self.learning_rate * parameter.grad\n",
    "            \n",
    "optimizer = Optimizer(model.parameters)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MSE:\n",
    "    \"\"\"Mean Squared Error (MSE) loss function.\"\"\"\n",
    "    \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"Computes the mean squared error between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            y_true: Iterable of true target values.\n",
    "            y_pred: Iterable of predicted values.\n",
    "\n",
    "        Returns:\n",
    "            The mean squared error as a float.\n",
    "        \"\"\"\n",
    "        return sum(((y_pred - y_true)**2 for y_true, y_pred in zip(y_true, y_pred))) / len(y_true)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'MSE()'\n",
    "    \n",
    "loss = MSE()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: Node(val=1.4228374421423402)\n",
      "Loss after: Node(val=1.4226470926950001)\n",
      "\n",
      "Loss before: Node(val=1.4226470926950001)\n",
      "Loss after: Node(val=1.4224567403775608)\n",
      "\n",
      "Loss before: Node(val=1.4224567403775608)\n",
      "Loss after: Node(val=1.4222663852823236)\n",
      "\n",
      "Loss before: Node(val=1.4222663852823236)\n",
      "Loss after: Node(val=1.4220760275016133)\n",
      "\n",
      "Loss before: Node(val=1.4220760275016133)\n",
      "Loss after: Node(val=1.421885667127775)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "# Data to work with\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5]\n",
    "      ]\n",
    "ys = [1.0, -1.0]\n",
    "\n",
    "\n",
    "# Loss function\n",
    "mse = MSE()\n",
    "\n",
    "# Optimization\n",
    "loss = mse(ys, [model(x) for x in xs])\n",
    "for _ in range(5):\n",
    "  print(f'Loss before: {loss}')\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  loss = mse(ys, [model(x) for x in xs])\n",
    "  print(f'Loss after: {loss}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: '(((((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2) + _((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)_) + ((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)) * _((((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2) + _((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)_) + ((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2))_)' | Value: 1.421885667127775 | Grad: 0\n",
      "Function: '_((((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2) + _((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)_) + ((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2))_' | Value: 0.5 | Grad: 0\n",
      "Function: '((((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2) + _((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)_) + ((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2))' | Value: 2.84377133425555 | Grad: 0\n",
      "Function: '((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)' | Value: 2.773108759354368 | Grad: 0\n",
      "Function: '(sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)' | Value: 1.6652653720516644 | Grad: 0\n",
      "Function: '_sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_' | Value: -1.0 | Grad: 0\n",
      "Function: 'sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))' | Value: 0.6652653720516645 | Grad: 0\n",
      "Function: '((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)' | Value: 0.6868479542449879 | Grad: 0\n",
      "Function: '(((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))))' | Value: 0.7218721856676892 | Grad: 0\n",
      "Function: '(neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))' | Value: 0.02286776720229652 | Grad: 0\n",
      "Function: 'sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))' | Value: 0.037172323264133916 | Grad: 0\n",
      "Function: '((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)' | Value: -3.254309965487125 | Grad: 0\n",
      "Function: '(((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_))' | Value: -3.2542574103155726 | Grad: 0\n",
      "Function: '(neuron_w1 * _neuron_w1_)' | Value: -0.5656716363496888 | Grad: 0\n",
      "Function: '_neuron_w1_' | Value: -1.0 | Grad: 0\n",
      "Function: '((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_)' | Value: -2.688585773965884 | Grad: 0\n",
      "Function: '_(neuron_w0 * _neuron_w0_)_' | Value: 0 | Grad: 0\n",
      "Function: '(neuron_w0 * _neuron_w0_)' | Value: -2.688585773965884 | Grad: 0\n",
      "Function: '_neuron_w0_' | Value: 3.0 | Grad: 0\n",
      "Function: '((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_)' | Value: 0.6990044184653926 | Grad: 0\n",
      "Function: '_(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_' | Value: 0 | Grad: 0\n",
      "Function: '(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))' | Value: 0.6990044184653926 | Grad: 0\n",
      "Function: 'sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))' | Value: 0.9011481014078556 | Grad: 0\n",
      "Function: '((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)' | Value: 2.210046862350042 | Grad: 0\n",
      "Function: '(((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_))' | Value: 2.2127713260405537 | Grad: 0\n",
      "Function: '(neuron_w1 * _neuron_w1_)' | Value: -0.6235380453193825 | Grad: 0\n",
      "Function: '_neuron_w1_' | Value: -1.0 | Grad: 0\n",
      "Function: '((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_)' | Value: 2.836309371359936 | Grad: 0\n",
      "Function: '_(neuron_w0 * _neuron_w0_)_' | Value: 0 | Grad: 0\n",
      "Function: '(neuron_w0 * _neuron_w0_)' | Value: 2.836309371359936 | Grad: 0\n",
      "Function: '_neuron_w0_' | Value: 3.0 | Grad: 0\n",
      "Function: '(((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2) + _((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)_)' | Value: 0.07066257490118202 | Grad: 0\n",
      "Function: '_((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)_' | Value: 0 | Grad: 0\n",
      "Function: '((sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)^2)' | Value: 0.07066257490118202 | Grad: 0\n",
      "Function: '(sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)) - _sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_)' | Value: -0.2658243309051713 | Grad: 0\n",
      "Function: '_sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))_' | Value: 1.0 | Grad: 0\n",
      "Function: 'sigmoid(((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b))' | Value: 0.7341756690948287 | Grad: 0\n",
      "Function: '((((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))) + b)' | Value: 1.0159126506873044 | Grad: 0\n",
      "Function: 'b' | Value: -0.03502423142270127 | Grad: 0.3189475734875923\n",
      "Function: '(((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_) + (neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))))' | Value: 1.0509368821100056 | Grad: 0\n",
      "Function: '(neuron_w1 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))' | Value: 0.2929260006040351 | Grad: 0\n",
      "Function: 'sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))' | Value: 0.47616104758271144 | Grad: 0\n",
      "Function: '((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)' | Value: -0.09542816209974223 | Grad: 0\n",
      "Function: 'b' | Value: -5.255517155253181e-05 | Grad: 0.00020938258707805385\n",
      "Function: '(((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_))' | Value: -0.0953756069281897 | Grad: 0\n",
      "Function: '(neuron_w1 * _neuron_w1_)' | Value: 1.6970149090490663 | Grad: 0\n",
      "Function: '_neuron_w1_' | Value: 3.0 | Grad: 0\n",
      "Function: 'neuron_w1' | Value: 0.5656716363496888 | Grad: -0.03202883777302976\n",
      "Function: '((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_)' | Value: -1.792390515977256 | Grad: 0\n",
      "Function: '_(neuron_w0 * _neuron_w0_)_' | Value: 0 | Grad: 0\n",
      "Function: '(neuron_w0 * _neuron_w0_)' | Value: -1.792390515977256 | Grad: 0\n",
      "Function: '_neuron_w0_' | Value: 2.0 | Grad: 0\n",
      "Function: 'neuron_w0' | Value: -0.896195257988628 | Grad: 0.008583011557722089\n",
      "Function: 'neuron_w1' | Value: 0.6151826196013073 | Grad: -0.0109004855883307\n",
      "Function: '((neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))) + _(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_)' | Value: 0.7580108815059705 | Grad: 0\n",
      "Function: '_(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))_' | Value: 0 | Grad: 0\n",
      "Function: '(neuron_w0 * sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)))' | Value: 0.7580108815059705 | Grad: 0\n",
      "Function: 'sigmoid(((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b))' | Value: 0.9772185249061045 | Grad: 0\n",
      "Function: '((((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_)) + b)' | Value: 3.7587625865075935 | Grad: 0\n",
      "Function: 'b' | Value: -0.0027244636905112895 | Grad: 0.024728966759279464\n",
      "Function: '(((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_) + (neuron_w1 * _neuron_w1_))' | Value: 3.761487050198105 | Grad: 0\n",
      "Function: '(neuron_w1 * _neuron_w1_)' | Value: 1.8706141359581476 | Grad: 0\n",
      "Function: '_neuron_w1_' | Value: 3.0 | Grad: 0\n",
      "Function: 'neuron_w1' | Value: 0.6235380453193825 | Grad: -0.02831096225958338\n",
      "Function: '((neuron_w0 * _neuron_w0_) + _(neuron_w0 * _neuron_w0_)_)' | Value: 1.8908729142399574 | Grad: 0\n",
      "Function: '_(neuron_w0 * _neuron_w0_)_' | Value: 0 | Grad: 0\n",
      "Function: '(neuron_w0 * _neuron_w0_)' | Value: 1.8908729142399574 | Grad: 0\n",
      "Function: '_neuron_w0_' | Value: 2.0 | Grad: 0\n",
      "Function: 'neuron_w0' | Value: 0.9454364571199787 | Grad: 0.07508239915291437\n",
      "Function: 'neuron_w0' | Value: 0.775682063107434 | Grad: 0.2834844024870324\n"
     ]
    }
   ],
   "source": [
    "loss.computational_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
